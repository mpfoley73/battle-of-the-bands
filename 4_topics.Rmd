---
title: "Text Mining Rush Lyrics"
subtitle: "Section 4: Topic Models"
author: "Michael Foley"
date: "`r Sys.Date()`"
output: 
  html_document:
    css: "style.css"
    theme: flatly
    toc: true
    toc_float: true
    highlight: haddock
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tidytext)
library(plotly)
library(glue)
library(stm)
library(janitor)
library(cluster) 

lyrics <- readRDS("./3_lyrics.Rds")
lyrics_lines <- readRDS("./1_lyrics_lines.Rds")

band_palette <- c("Queen" = "lightgoldenrod", "Rush" = "darkseagreen", "AC/DC" = "slategray1")
writer_palette <- c(
  "Queen" = "lightgoldenrod", 
  "Brian May" = "lightsalmon",
  "Freddie Mercury" = "thistle", 
  "Roger Taylor" = "lavender",
  "John Deacon" = "lightpink", 
  "Neil Peart" = "darkseagreen", 
  "AC/DC" = "slategray1"
)
```

> Section 4 of my [Battle of the Bands](https://mpfoley73.github.io/battle-of-the-bands/) text mining project compares recurring topics in songs by the writers from Queen, Rush, and AC/DC. Topic models identify dominant themes in text and are useful tools for summarizing large corposes. The topic model weights are useful inputs into a cluster analysis that finds similar songs. Fan of Freddie Mercury's The Fairy Feller's Master-Stroke? Try Neil Peart's Anthem - their lyrics clustered together!

## Background

Topic modeling searches for patterns of co-occurring words in a corpus of documents. Most topic models represent documents as the output of a probabilistic data-generating mechanism. The model optimizes the distribution parameters according to some criteria. The data generating mechanism is a pair of probability distributions.

The first probability distribution defines topics as weighted probabilities of terms from the vocabulary vector. The second probability distribution defines documents as weighted probabilities of topics. You might interpret the topic weights as _the probability that document x is about topic y_, or as _the degree of the association between document x and topic y_, or as _how much this topic y contributed to document x_.

STM differs from other topic models in that you can fit the model with controlling variables so that a) the first probability distribution (sometimes called the beta matrix) is a function of the controlling variables (a *topical content* model) and/or b) the second probability distribution (sometimes called the gamma matrix) is a function of the controlling variables (a *topical prevalence* model). If you think _how_ a topic is discussed depends on the metadata features, control for them in a topical content model. E.g., middle-school children will discuss a topic differently form doctoral candidates. If you think _what_ topics are expressed depends on the metadata features, control for them in a topical prevalence model. E.g., a survey with and open text comment accompanying a Likert rating will probably focus on different topics for each rating level. 

That is my approach here. I assume there is overlap among the song writers, but that some topics are more strongly associated with some writers than others. I wonder if I can use topic models to identify similar songs.

My `lyrics` data frame consists of `r nrow(lyrics)` songs and `r ncol(lyrics)` columns. After the first eight descriptive attributes columns are 8 columns of features engineered in the [text complexity](file:///C:/GitHub/battle-of-the-bands/3_complexity.html) section.

```{r}
glimpse(lyrics)
```

## N-Grams

A good first approach to analyzing lyrical content is to construct unigram and bigram counts. The **tidytext** provides an easy framework for tokenizing text into unigrams, pulling out connector words and other common "stop" words, and then taking summary statistics (usually just counts). Tidytext tokenizes into bigrams too, but bigrams are less insightful if one of the words is a stop word. One option is to tokenize into unigrams, remove stop words, reassemble into a stop-free document, then tokenize into bigrams.

```{r}
lyrics_tidy <- lyrics %>%
  
  # remove special chars
  mutate(lyrics_tidy = str_remove_all(lyrics, "[[:punct:]]")) %>%
  
  # create unigrams
  unnest_tokens(output = "word", input = lyrics_tidy) %>%
  
  # no misspellings here, so I'm skipping this step
  # left_join(
  #   fuzzyjoin::misspellings %>% distinct(misspelling, .keep_all = TRUE),
  #   by = c("word" = "misspelling")
  # ) %>%
  # mutate(word = coalesce(correct, word)) %>%
  # select(-correct) %>%

  # lemmatize words 
  mutate(word = textstem::lemmatize_words(word, dictionary = lexicon::hash_lemmas)) %>%
  
  # remove stop words
  anti_join(stop_words, by = "word") %>%
  
  # reconstruct the lyrics 
  nest(word_list = word) %>%
  mutate(lyrics_tidy = map_chr(word_list, ~ unlist(.) %>% paste(., collapse = " "))) %>%
  select(-word_list)
```

`lyrics_tidy` has a new column, `lyrics_tidy` that has stop words removed, and words lemmatized. Here is what Freddie Mercury's Bohemian Rhapsody looks like before and after this processing.

```{r}
lyrics_tidy %>%
  filter(song == "Bohemian Rhapsody") %>%
  pivot_longer(cols = c(lyrics, lyrics_tidy)) %>%
  select(song, value) %>%
  head() %>%
  flextable::as_grouped_data("song") %>% 
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::theme_vanilla() %>%
  flextable::bg(i = ~ !is.na(song), bg = "gray80") %>%
  flextable::border(i = c(2), border.bottom = officer::fp_border()) %>%
  flextable::border(j = 2, border.right = officer::fp_border())

```

### Unigrams

Unnesting tokens into unigrams with counts gives the following "top 5" words for each writer.

```{r}
tidy_top5_plot <- function(x, title_text) {
  x %>%
    group_by(band, writer) %>%
    count(token) %>%
    mutate(token_pct = n / sum(n)) %>%
    slice_max(order_by = token_pct, n = 5, with_ties = FALSE) %>%
    mutate(token = reorder_within(token, token_pct, writer)) %>%
    ggplot(aes(x = token, y = token_pct, fill = band)) +
    geom_col() +
    scale_color_manual(values = band_palette) +
    scale_x_reordered() +
    scale_y_continuous(labels = scales::percent_format(accuracey = 1)) +
    facet_wrap(facets = vars(writer), scales = "free_y") +
    coord_flip() +
    theme_light() +
    theme(legend.position = "none") +
    labs(x = NULL, y = "frequency", title = title_text)
}

# This is a first attempt at a word count. A better one comes later.
lyrics_word <- lyrics_tidy %>%
  unnest_tokens(output = "token", input = lyrics_tidy, token = "words")

lyrics_word %>% tidy_top5_plot("Top 5 Words (a first look).")
```

"I'm" is a top-5 word for every writer except Neil Peart. Peart rarely wrote explicity about himself. There are some gibberish words like "ooh" that don't add much insight. I'll remove them and try again.

```{r}
custom_stop_words <- data.frame(token = c("ooh", "whoa"))

# This is the final word count
lyrics_word_2 <- lyrics_word %>%
  anti_join(custom_stop_words, by = "token")

lyrics_word_2 %>% tidy_top5_plot("Top 5 Words (improved).")
```

"Love" is another prevalent word, appearing in the top-5 for everyone except Roger Taylor and Queen. AC/DC was distinctive with reference to rock 'n roll. Peart had a relatively low word frequency for his top-5, suggesting he had a wider range of words and themes in songs.

### Bigrams 

Bigrams are easier to interpret, but they also can mask ideas with variation in phrasing. Here are the bigrams pulled from the distilled `lyrics_tidy` column.

```{r}
lyrics_bigram <- lyrics_tidy %>%
  unnest_tokens(output = "token", input = lyrics_tidy, token = "ngrams", n = 2) %>%
  filter(!is.na(token))

lyrics_bigram %>% tidy_top5_plot("Top 5 Bigrams")
```

AC/DC is all about rock 'n roll. Brian May's songs [It's Late](https://genius.com/Queen-its-late-lyrics), [Dancer](https://genius.com/Queen-dancer-lyrics), and [Sweet Lady](https://genius.com/Queen-sweet-lady-lyrics) are evident in his list. The same is true for Deacon, Queen, and Neil Peart. Songs with repeating phrases appear in the bigrams - not very interesting.

## Modeling

I will fit a structural topic model (STM) following the procedure from the [stm vignette](https://www.structuraltopicmodel.com/).]

### Process and Prepare

Process the data first. Some of this is overlap with the processing I've just completed.

```{r}
processed <- stm::textProcessor(
  lyrics_tidy$lyrics_tidy,
  metadata = lyrics_tidy,
  stem = FALSE,
  customstopwords = custom_stop_words$token
)
```

`textProcessor()` produces a list object with three main components:

* a `vocab` named vocabulary vector. The vector has `r length(processed$vocab) %>% scales::comma()` words.
* a `documents` list of matrices, one per document. Each matrix has 2 rows of integers. The first row is indices from the vocabulary vector; the second is their associated word counts. This is a concise representation of a document term matrix. The processing step sometimes removes a few documents that are empty after removing chaff. However, I still have all `r processed$documents %>% length() %>% scales::comma()` rows in my `documents` list.
* a `meta` metadata data frame. There is one row per document (`r nrow(processed$meta)` rows) containing all the song features I've collected in prior sections.

Next, "prepare" the corpus by removing infrequently used words. You can leave all words in, but in improves performance to cull out words with such low frequencies that they are unlikely to contribute to topics. The following diagnostic plot helps.

```{r}
stm::plotRemoved(
  processed$documents, 
  lower.thresh = seq(1, length(processed$documents), by = 10)
)
```

If you remove words appearing in less than half the songs, all songs will be empty. 1% is a conservative threshold. Only words appearing in at least 1% of songs in the corpus will be included in a topic. `prepDocuments()` removes words with frequencies below the defined threshold, then updates the vocabulary, documents, and metadata.

```{r}
prepared <- stm::prepDocuments(
  processed$documents,
  processed$vocab,
  processed$meta,
  lower.thresh = length(processed$documents) * .01
)
```

I didn't lose any songs - I'm still at `r prepared$documents %>% length() %>% scales::comma()` songs. The vocabulary vector shrank from `r length(processed$vocab) %>% scales::comma()` words to `r length(prepared$vocab) %>% scales::comma()` words. Here are the first 100, just to get a sense.

```{r}
prepared$vocab[1:100]
```

### Fit the Model

The **stm** package allows you to either specify the number of topics (K) to identify, or it can choose an optimal number by setting parameter `K = 0`. I'll let **stm** choose. The resulting probability distribution of topic words will then be a K x `r length(prepared$vocab)` matrix, sometimes called the _beta_matrix_. The probability distribution of song topics will be a `r length(prepared$documents) %>% scales::comma()` x K matrix, sometimes called the _gamma_matrix_ (*theta* in the **stm** package).
 
I expect lyrics to be correlated with the writer, so I will fit a *prevalence* model with `writer` as a covariate.

```{r}
set.seed(1234)

fit_prevalence <- stm::stm(
  documents = prepared$documents,
  vocab = prepared$vocab,
  K = 0,
  prevalence = ~ writer,
  data = prepared$meta,
  init.type = "Spectral",
  verbose = FALSE
)

summary(fit_prevalence)
```

### Interpretation

`stm()` produced a model with 
The model summary printed above (you can also print it with `stm::labelTopics(fit_prevalence)`) shows the top words based on four metrics: highest probability, FREX, lift, and score.

* **Highest Probability** weights words by their overall frequency.
* **FREX** weights words by their overall frequency and how exclusive they are to the topic.
* **Lift** weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics.
* **Score** divides the log frequency of the word in the topic by the log frequency of the word in other topics.

Let's look at Topic 2:

```{r}
stm::labelTopics(fit_prevalence, topics = 2)
```

`findThoughts()` shows comments that mapped highly to the topic. The top 3 mappings are below. 

```{r}
topic_thoughts <- stm::findThoughts(
  fit_prevalence, 
  n = 3, 
  texts = processed$meta$lyrics, 
  topics = 2,
  meta = processed$meta
)

processed$meta[pluck(topic_thoughts$index, 1), ] %>% select(writer, song, song_url)
```

## Model Exploration

For reporting purposes, you might want to sum each topic up with a title. That's not going to be useful here with `r fit_prevalence$theta %>% ncol()` topics. Instead, I'll use the top-5 *Highest Probability* words.

```{r}
topic_lbl <- stm::labelTopics(fit_prevalence, n = 5) %>% 
  pluck("prob") %>%
  as.data.frame() %>%
  mutate(
    topic_num = row_number(),
    topic_id = paste0("topic_", topic_num)
  ) %>%
  rowwise() %>%
  mutate(
    topic = paste(c(V1, V2, V3, V4, V5), collapse = ", "),
    topic_long = paste0(topic_num, ": ", topic)
  ) %>%
  select(topic_id, topic, topic_long)

topic_df <- fit_prevalence$theta %>% 
  as.data.frame() 
colnames(topic_df) <- topic_lbl$topic_id

lyrics_tidy_2 <- processed$meta %>%
  bind_cols(topic_df)
```

Data frame `lyrics_tidy_2` has the `r fit_prevalence$theta %>% ncol()` topic weights attached as columns with names topic_1, topic_2, ..., topic_`r fit_prevalence$theta %>% ncol()`. The topic descriptors are in data frame `topic_lbl` which I can use as a lookup after pivoting `lyrics_tidy_2`.

```{r}
lyrics_tidy_2_long <- lyrics_tidy_2 %>% 
  pivot_longer(
    cols = starts_with("topic_"), 
    names_to = "topic_id", 
    values_to = "topic_weight"
  ) %>%
  inner_join(topic_lbl, by = "topic_id") %>%
  mutate(
    topic = factor(topic, levels = topic_lbl$topic),
    topic_long = factor(topic_long, levels = topic_lbl$topic_long)
  ) 
```

Each song is a mix of topics, so for each song the topic weights sum to 1. 

Topics usually make a negligible contribution to most songs, and a substantial contribution to a few songs. Viewed from the other side, most songs are composed primarily of one or two topics.

```{r fig.height=8, fig.width=6}
p <- lyrics_tidy_2_long %>%
  ggplot(aes(x = fct_rev(topic_long), y = topic_weight, color = writer,
             text = glue("song: {song} <br>",
                         "topic: {topic} <br>",
                         "weight: {scales::percent(topic_weight, accuracy = 1)}"))) +
  geom_point(alpha = .8) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_color_manual(values = writer_palette) +
  coord_flip() +
  theme_light() +
  labs(
    title = "Topics usually make up <10% of a song",
    subtitle = "Song Weights for each topic.",
    x = NULL, y = "Topic Weight")
ggplotly(p, tooltip = "text")
```

Here are the top topics for _We Will Rock You_.

```{r}
lyrics_tidy_2_long %>%
  filter(song == "We Will Rock You") %>%
  arrange(desc(topic_weight)) %>%
  mutate(topic_weight = topic_weight * 100) %>%
  head() %>%
  select(topic_long, topic_weight) %>%
  flextable::flextable() %>%
  flextable::colformat_double(digits = 0, suffix = "%") %>%
  flextable::autofit() %>%
  flextable::set_caption("We Will Rock You topics")
```

A heat map of topic importance shows a higher topic concentration Deacon, Taylor, and Queen, but that is likely due to their small corpus sizes. Neil Peart weighed heavily on topic 15 (`r topic_lbl %>% filter(topic_id == "topic_15") %>% pull(topic)`) while other writers used it hardly at all. AC/DC relied on topic 32 (`r topic_lbl %>% filter(topic_id == "topic_32") %>% pull(topic)`), and so did Roger Taylor.

```{r fig.height=8, fig.width=6}
topic_importance <- lyrics_tidy_2_long %>%
  group_by(writer, topic_long) %>%
  summarize(
    .groups = "drop",
    songs = n(),
    important = sum(topic_weight >= .25),
    pct_import = important / songs)

topic_importance %>%
  ggplot(aes(x = writer, y = topic_long, label = pct_import)) +
  geom_tile(aes(fill = pct_import)) +
  scale_fill_gradient(low = "#FFFFFF", high = "goldenrod") +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 8)) +
  theme_light() +
  theme(legend.position = "top") +
  labs(title = "Topic Importance by Writer", fill = NULL, y = NULL, x = NULL)
```

Which writers were most similar? Topic weights for AC/DC were highly correlated with Roger Taylor, but negatively correlated with the other writers, especially Neil Peart. Peart was no correlated with anyone. The most similar writers were Brian May and Freddie Mercury. John Deacon was in between, correlated less strongly to both Mercury and May.

```{r}
topic_importance %>% 
  select(writer, topic_long, pct_import) %>%
  pivot_wider(names_from = writer, values_from = pct_import) %>%
  column_to_rownames(var = "topic_long") %>%
  cor() %>%
  corrplot::corrplot(type = "upper")
```

## Similar Songs

A cluster analysis can use the topics to find similar songs. I'll follow the steps from my [unsupervised learning handbook](https://bookdown.org/mpfoley1973/unsupervised-ml/cluster-analysis.html) chapter on K-mediods cluster analysis.

The first step is to calculate the song distances from each other. I use the Gower distance.

```{r}
lyrics_gower <- lyrics_tidy_2 %>% 
  select(-c(song_id:lyrics, lyrics_tidy)) %>%
  cluster::daisy(metric = "gower")
```

Let’s see the most similar and dissimilar pairs of songs according to their Gower distance.

```{r}
x <- as.matrix(lyrics_gower)
bind_rows(
  lyrics_tidy_2[which(x == min(x[x != 0]), arr.ind = TRUE)[1, ], ],
  lyrics_tidy_2[which(x == max(x[x != 0]), arr.ind = TRUE)[1, ], ]
) %>%
  as.data.frame() %>%
  select(-c(lyrics_tidy, topic_1:topic_39)) %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(cols = -song) %>%
  pivot_wider(names_from = song) %>%
  flextable::flextable() %>%
  flextable::add_header_row(values = c("", "Similar", "Dissimilar"), colwidths = c(1, 2, 2)) %>%
  flextable::border(j = c(1, 3), border.right = officer::fp_border("gray80"), part = "all")
```

The K-means algorithm randomly assigns all observations to one of _K_ clusters. K-means iteratively calculates the cluster centroids and reassigns observations to their nearest centroid. Centroids are set of mean values for each feature (hence the name "K-*means*"). The iterations continue until either the centroids stabilize or the iterations reach a set maximum (typically 50). The result is _K_ clusters with the minimum total intra-cluster variation.

What value should _K_ take? Construct a *silhouette plot*. 

```{r warning=FALSE, message=FALSE}
set.seed(1234)

pam_mdl <- data.frame(k = 2:100) %>%
  mutate(
    mdl = map(k, ~pam(lyrics_gower, k = .)),
    sil = map_dbl(mdl, ~ .$silinfo$avg.width)
  )

pam_mdl %>%
  ggplot(aes(x = k, y = sil)) +
  geom_point(size = 2) +
  geom_line() +
  geom_vline(aes(xintercept = 47), linetype = 2, size = 1, color = "goldenrod") +
  scale_x_continuous(breaks = seq(0, 100, by = 5)) +
  theme_light() +
  labs(title = "Silhouette plot max occurs at K = 47 clusters.", 
       subtitle = "K-Medoids within-cluster average silhouette width at candidate values of K.", 
       y = "")
```

Attach the results to the original table for visualization and summary statistics.  

```{r}
pam_mdl_final <- pam_mdl %>% filter(k == 47) %>% pluck("mdl", 1)

lyrics_tidy_3 <- lyrics_tidy_2 %>% 
  mutate(cluster = as.factor(pam_mdl_final$clustering))
```

If each cluster were an album, which songs would be on them? Use the filter below to change the albums.

```{r}
lyrics_tidy_3 %>%
  select(cluster, band, writer, song) %>%
  arrange(cluster, band, writer, song) %>%
  DT::datatable(
    filter = "top",
    options = list(pageLength = 20)
  )
```

## Save work

Save the lyrics with complexity stats for subsequent steps.

```{r}
saveRDS(lyrics_tidy_3, "./4_lyrics.Rds")
saveRDS(fit_prevalence, "./4_model.Rds")
saveRDS(lyrics_tidy_2_long, "./4_lyrics_long.Rds")
```
